{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym Warm-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space of the Env:  16\n",
      "State space of the Env by accessing env.nS:  16\n",
      "Action space of the Env:  4\n"
     ]
    }
   ],
   "source": [
    "# Import Environment class and Libraries\n",
    "from frozen_lake import FrozenLakeEnv\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Create Environment Object\n",
    "env = FrozenLakeEnv(map_name =\"4x4\", is_slippery=False)\n",
    "\n",
    "\n",
    "# Access the number of states:\n",
    "nS = env.observation_space\n",
    "print(\"State space of the Env: \", nS)\n",
    "\n",
    "# or you could even use \n",
    "nS = env.nS\n",
    "print(\"State space of the Env by accessing env.nS: \", nS)\n",
    "\n",
    "\n",
    "# Action space of the agent:\n",
    "nA = env.nA\n",
    "print(\"Action space of the Env: \", nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilty:  1.0\n",
      "Next State:  8\n",
      "Reward:  0.0\n",
      "Episode ended? :  False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For policy iteration, you would need to access\n",
    "State(s), Action(a), Next State(ns), Reward(r), episode ended? (is_done) tuples.\n",
    "\n",
    "Note that in this environment, the orientation of the agent does not matter.\n",
    "No matter what direction the agent is facing, if, say a left action is performed, \n",
    "the agent moves to the left of the crrent state.\n",
    "\"\"\"\n",
    "\n",
    "# For actions, this is the corresponding dictionary:\n",
    "action_names = {0:'L', 1:'D', 2:\"R\", 3:\"U\"}\n",
    "\n",
    "\"\"\"\n",
    "Here, \n",
    "'L' means left\n",
    "'D' means down\n",
    "'R' means right\n",
    "'U' means up\n",
    "\n",
    "\n",
    "\n",
    "You can access these tuples by simply env.P[s][a].\n",
    "where 's' is state, and 'a' is action. For example, let's say we are at state '4',\n",
    "and we take an action '1' or \"Down\". The next state (ns) would be 8, the episode would not have ended (is_done), \n",
    "the reward (r) is 0 and the transition probabilty (prob) is 1 because this is a deterministic setting.\n",
    "\"\"\"\n",
    "\n",
    "prob, ns, r, is_done = env.P[4][1][0]\n",
    "\n",
    "\n",
    "print(\"Transition Probabilty: \", prob)\n",
    "print(\"Next State: \", ns)\n",
    "print(\"Reward: \", r)\n",
    "print(\"Episode ended? : \", is_done)\n",
    "# Note that we need to add a [0] after env.P[s][a] because it returns a list containing the tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration \n",
    "\n",
    "- Follow the pseudo-code given in the handout for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, action_names, states):\n",
    "    \"\"\"Print the policy in human-readable format.\n",
    "    If you've implemented this correctly, the output (for 4x4 map) should be:\n",
    "    \n",
    "    D R D L \n",
    "    D L D L \n",
    "    R D D L \n",
    "    L R R L \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.ndarray\n",
    "        Array of state to action number mappings\n",
    "    action_names: dict\n",
    "        Mapping of action numbers to characters representing the action.\n",
    "    num_states: int\n",
    "        Number of states in the FrozenLakeEnvironment (16 or 64 for 4x4 or 8x8 maps respectively)      \n",
    "    \"\"\"\n",
    "    \n",
    "    # WRITE YOUR CODE HERE:\n",
    "    side = int(np.sqrt(states))\n",
    "    arr = np.chararray((side, side), unicode = 'True')\n",
    "    for i in range(side):\n",
    "        test = np.chararray(side)\n",
    "        for j in range(side):\n",
    "            test[j] = action_names[policy[i*side+j]]\n",
    "        arr[i,:] = test\n",
    "    print(arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_sync(env, gamma, policy, value_func, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "    \n",
    "    Evaluates the value of a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to compute value iteration for.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    value_func: np.array\n",
    "      Array of scalar values for each state\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "\n",
    "    val_iter = 0\n",
    "    # WRITE YOUR CODE HERE:\n",
    "    delta = 0\n",
    "    for i in range(max_iterations):\n",
    "        for s in range(env.nS):\n",
    "            a = policy[s]\n",
    "            _, ns, r, _ = env.P[s][a][0]\n",
    "            v = r + gamma*(value_func[int(ns)])\n",
    "            delta = max(delta, np.abs(v - value_func[s]))\n",
    "            value_func[s] = v\n",
    "        val_iter += 1\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return value_func, val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(env, gamma, value_func, policy):\n",
    "    \"\"\"Performs policy improvement.\n",
    "    \n",
    "    Given a policy and value function, improves the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to compute value iteration for.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    value_func: np.ndarray\n",
    "      Value function for the given policy.\n",
    "    policy: dict or np.array\n",
    "      The policy to improve. Maps states to actions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool, np.ndarray\n",
    "      Returns the new imporved policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # WRITE YOUR CODE HERE:\n",
    "    new_policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        a_val = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            _, ns, r, _ = env.P[s][a][0]\n",
    "            a_val[a] = r + gamma*(value_func[ns])\n",
    "        new_policy[s] = np.argmax(a_val)\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_sync(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    See page 85 of the Sutton & Barto Second Edition book.\n",
    "\n",
    "    You should call the improve_policy() and evaluate_policy_sync() methods to\n",
    "    implement this method.\n",
    "    \n",
    "    If you've implemented this correctly, it should take much less than 1 second.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env: Frozen Lake Environment\n",
    "      The environment to compute value iteration for.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = np.random.randint(0, 4, size=env.nS)   #Define random policy\n",
    "    value_func_init = np.zeros(env.nS)    # Define initial value function\n",
    "    num_pol_iter = 0\n",
    "    num_val_iter = 0\n",
    "    value_func_list = []\n",
    "    # WRITE YOUR CODE HERE:\n",
    "    for i in range(max_iterations):\n",
    "        val_func, val_iter = evaluate_policy_sync(env, gamma, policy, value_func_init)\n",
    "        num_val_iter += 1\n",
    "        new_policy = improve_policy(env, gamma, val_func, policy)\n",
    "        num_pol_iter += 1\n",
    "        value_func_list.append(val_func)\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "        policy = new_policy\n",
    "        value_func_init = val_func\n",
    "    return policy, value_func_init, num_pol_iter, num_val_iter,value_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR 4x4 FROZEN LAKE\n",
      "[['D' 'R' 'D' 'L']\n",
      " ['D' 'L' 'D' 'L']\n",
      " ['R' 'D' 'D' 'L']\n",
      " ['L' 'R' 'R' 'L']]\n",
      "Time elapsed for 4x4 map: 0.3823 seconds\n",
      "Number of policy improvement steps: 7 steps\n",
      "Number of policy evaluation stpes: 7 steps\n",
      "\n",
      "RESULTS FOR 8x8 FROZEN LAKE\n",
      "[['D' 'D' 'D' 'D' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'R' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'L' 'D' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D' 'L' 'D' 'D']\n",
      " ['R' 'R' 'U' 'L' 'D' 'D' 'R' 'D']\n",
      " ['D' 'L' 'L' 'R' 'R' 'D' 'L' 'D']\n",
      " ['D' 'L' 'R' 'U' 'L' 'D' 'L' 'D']\n",
      " ['R' 'R' 'U' 'L' 'R' 'R' 'R' 'L']]\n",
      "Time elapsed for 4x4 map: 3.6084 seconds\n",
      "Number of policy improvement steps: 14 steps\n",
      "Number of policy evaluation stpes: 14 steps\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # WRITE YOUR CODE HERE:\n",
    "    print(\"RESULTS FOR 4x4 FROZEN LAKE\")\n",
    "    start = time.time()\n",
    "    env = FrozenLakeEnv(map_name =\"4x4\", is_slippery=False)\n",
    "    pol, val_func, pol_it, val_it, val_list = policy_iteration_sync(env, 0.9)\n",
    "    arr = print_policy(pol, action_names, 16)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed for 4x4 map: {} seconds\".format(np.round(end-start, 4)))\n",
    "    print(\"Number of policy improvement steps: {} steps\".format(pol_it))\n",
    "    print(\"Number of policy evaluation stpes: {} steps\".format(val_it))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"RESULTS FOR 8x8 FROZEN LAKE\")\n",
    "    start = time.time()\n",
    "    env = FrozenLakeEnv(map_name =\"8x8\", is_slippery=False)\n",
    "    pol, val_func, pol_it, val_it, val_list = policy_iteration_sync(env, 0.9)\n",
    "    arr = print_policy(pol, action_names, 64)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed for 4x4 map: {} seconds\".format(np.round(end-start, 4)))\n",
    "    print(\"Number of policy improvement steps: {} steps\".format(pol_it))\n",
    "    print(\"Number of policy evaluation stpes: {} steps\".format(val_it))\n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
